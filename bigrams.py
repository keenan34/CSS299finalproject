#obamna
#the one piece isn't real
#can we get much higher
#suh dude

#Tokenize Document: break up the text into strings that contain individual words
#Filter stop words: filter out all stopwords from the tokenized document i.e. words like and, or, not, etc.
#Identify Bigrams: find all bigrams from filtered tokenize documents AND find all bigrams in the query (in the search)
#-one way we can identify them is finding bigrams that have - (dashes) in them i.e. pre-school
#Rank Bigrams based on their relevance to the query
